{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "# 对比四个简单的分类器\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# select k best features\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# import models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "data_label = pd.read_excel(\"./feature/class.xlsx\", index_col=1)\n",
    "data_label = data_label[data_label['exclusion'] == 1]\n",
    "# index去掉.npy\n",
    "data_label.index = data_label.index.str.replace('.npy', '')\n",
    "\n",
    "tongji_label = data_label[data_label['dataset'] == 'tongji'][\"class\"]\n",
    "xiangyang_label = data_label[data_label['dataset'] == 'xiangyang'][\"class\"]\n",
    "kits_label = data_label[data_label['dataset'] == 'kits'][\"class\"]\n",
    "henan_label = data_label[data_label['dataset'] == 'henan'][\"class\"]\n",
    "\n",
    "tongji_rad = pd.read_excel('./feature/rad/rad_tongji.xlsx', index_col=0).iloc[:, 37:]\n",
    "xiangyang_rad = pd.read_excel('./feature/rad/rad_xiangyang.xlsx', index_col=0).iloc[:, 37:]\n",
    "kits_rad = pd.read_excel('./feature/rad/rad_kits.xlsx', index_col=0).iloc[:, 37:]\n",
    "henan_rad = pd.read_excel('./feature/rad/rad_henan.xlsx', index_col=0).iloc[:, 37:]\n",
    "\n",
    "# 合并数据表格\n",
    "feature_rad = pd.concat([tongji_rad, xiangyang_rad], axis=0)\n",
    "label = pd.concat([tongji_label, xiangyang_label], axis=0)\n",
    "\n",
    "# 按照index排序\n",
    "feature_rad = feature_rad.sort_index()\n",
    "label = label.sort_index()\n",
    "\n",
    "kits_rad = kits_rad.sort_index()\n",
    "kits_label = kits_label.sort_index()\n",
    "\n",
    "henan_rad = henan_rad.sort_index()\n",
    "henan_label = henan_label.sort_index()\n",
    "\n",
    "assert all(feature_rad.index == label.index)\n",
    "assert all(kits_rad.index == kits_label.index)\n",
    "assert all(henan_rad.index == henan_label.index)\n",
    "\n",
    "train_test_index = pd.read_excel(\"./index.xlsx\", index_col=0)\n",
    "train_index = train_test_index[train_test_index.index == 'train']['name'].values.ravel()\n",
    "val_index = train_test_index[train_test_index.index == 'val']['name'].values.ravel()\n",
    "X_train = feature_rad.loc[train_index]\n",
    "y_train = label.loc[train_index]\n",
    "X_val = feature_rad.loc[val_index]\n",
    "y_val = label.loc[val_index]\n",
    "\n",
    "# # 定义 X_train, y_train, X_val, y_val\n",
    "# X_train, X_val, y_train, y_val = train_test_split(feature_rad, label, test_size=0.3, stratify=label, random_state=42)\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_val = pd.DataFrame(scaler.transform(X_val), index=X_val.index, columns=X_val.columns)\n",
    "X_henan = pd.DataFrame(scaler.transform(henan_rad), index=henan_rad.index, columns=henan_rad.columns)\n",
    "X_kits = pd.DataFrame(scaler.transform(kits_rad), index=kits_rad.index, columns=kits_rad.columns)\n",
    "y_henan = henan_label\n",
    "y_kits = kits_label\n",
    "assert all(X_kits.index == y_kits.index)\n",
    "assert all(X_henan.index == y_henan.index)\n",
    "y_train = y_train.values.ravel()\n",
    "y_val = y_val.values.ravel()\n",
    "y_henan = y_henan.values.ravel()\n",
    "y_kits = y_kits.values.ravel()\n",
    "# print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(metric_func, y_prob, y_true, n_iterations=1000, ci=0.95):\n",
    "    \"\"\"\n",
    "    计算bootstrap置信区间\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    size = len(y_true)\n",
    "    scores = []\n",
    "    \n",
    "    rng = np.random.RandomState(42)  # 设置随机种子\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # 随机抽样（使用替换）\n",
    "        indices = rng.randint(0, size, size=size)\n",
    "        score = metric_func(y_true[indices], y_prob[indices])\n",
    "        scores.append(score)\n",
    "    \n",
    "    # 计算置信区间\n",
    "    sorted_scores = np.sort(scores)\n",
    "    alpha = (1 - ci) / 2\n",
    "    lower_bound = sorted_scores[int(alpha * n_iterations)]\n",
    "    upper_bound = sorted_scores[int((1 - alpha) * n_iterations)]\n",
    "    \n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "# def bootstrap_ci(metric_func, y_prob, y_true, n_iterations=1000, ci=0.95):\n",
    "#     \"\"\"\n",
    "#     使用bootstrap方法计算指标的置信区间\n",
    "#     \"\"\"\n",
    "#     scores = []\n",
    "#     size = len(y_true)\n",
    "    \n",
    "#     for _ in range(n_iterations):\n",
    "#         # 随机抽样\n",
    "#         indices = np.random.randint(0, size, size)\n",
    "#         score = metric_func(y_true[indices], y_prob[indices])\n",
    "#         scores.append(score)\n",
    "    \n",
    "#     # 计算置信区间\n",
    "#     lower = np.percentile(scores, ((1-ci)/2)*100)\n",
    "#     upper = np.percentile(scores, (1-(1-ci)/2)*100)\n",
    "    \n",
    "#     return round(lower, 3), round(upper, 3)\n",
    "\n",
    "def calculate_metrics(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    计算不平衡二分类问题的评价指标，包括95%置信区间\n",
    "    \n",
    "    参数：\n",
    "    y_prob: numpy array, 预测为正类(少数类)的概率值\n",
    "    y_true: numpy array, 真实标签 (0为多数类，1为少数类)\n",
    "    \n",
    "    返回：\n",
    "    dict: 包含评价指标和置信区间的字典\n",
    "    \"\"\"\n",
    "    # 1. AUC\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    auc_ci = bootstrap_ci(roc_auc_score, y_prob, y_true)\n",
    "    \n",
    "    # 2. 通过优化F1-score选择最佳阈值\n",
    "    # precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    # f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "    # f1_scores = np.nan_to_num(f1_scores)\n",
    "    # best_threshold = thresholds[np.argmax(f1_scores[:-1])]\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    # 使用最佳阈值进行预测\n",
    "    y_pred = (y_prob >= best_threshold).astype(int)\n",
    "    \n",
    "    # 3. F1-score\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    f1_ci = bootstrap_ci(lambda y_t, y_p: f1_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                        y_prob, y_true)\n",
    "    \n",
    "    # 4. ACC\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    acc_ci = bootstrap_ci(lambda y_t, y_p: accuracy_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                         y_prob, y_true)\n",
    "    \n",
    "    # 5. MCC\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    mcc_ci = bootstrap_ci(lambda y_t, y_p: matthews_corrcoef(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                         y_prob, y_true)\n",
    "    \n",
    "    # 6. AUPRC\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    def auprc_score(y_t, y_p):\n",
    "        p, r, _ = precision_recall_curve(y_t, y_p)\n",
    "        return auc(r, p)\n",
    "    \n",
    "    auprc_ci = bootstrap_ci(auprc_score, y_prob, y_true)\n",
    "    \n",
    "    # 7. Precision\n",
    "    precisionscore = precision_score(y_true, y_pred)\n",
    "    precision_ci = bootstrap_ci(lambda y_t, y_p: precision_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                              y_prob, y_true)\n",
    "\n",
    "    # 8. Recall\n",
    "    recallscore = recall_score(y_true, y_pred)\n",
    "    recall_ci = bootstrap_ci(lambda y_t, y_p: recall_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                            y_prob, y_true)\n",
    "\n",
    "    metrics = {\n",
    "        'AUC': round(auc_score, 3),\n",
    "        'AUC_CI': auc_ci,\n",
    "        'F1': round(f1, 3),\n",
    "        'F1_CI': f1_ci,\n",
    "        'ACC': round(acc, 3),\n",
    "        'ACC_CI': acc_ci,\n",
    "        'MCC': round(mcc, 3),\n",
    "        'MCC_CI': mcc_ci,\n",
    "        'AUPRC': round(auprc, 3),\n",
    "        'AUPRC_CI': auprc_ci,\n",
    "        # 'threshold': round(best_threshold, 3),\n",
    "        'Precision': round(precisionscore, 3),\n",
    "        'Precision_CI': precision_ci,\n",
    "        'Recall': round(recallscore, 3),\n",
    "        'Recall_CI': recall_ci\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def format_metrics(metrics):\n",
    "    \"\"\"\n",
    "    格式化指标输出，包含置信区间\n",
    "    \"\"\"\n",
    "    formatted = {}\n",
    "    for key in metrics:\n",
    "        if key.endswith('_CI'):\n",
    "            continue\n",
    "        if key + '_CI' in metrics:\n",
    "            formatted[key] = f\"{metrics[key]} ({metrics[key+'_CI'][0]}-{metrics[key+'_CI'][1]})\"\n",
    "        else:\n",
    "            formatted[key] = f\"{metrics[key]}\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "def merge_dicts_to_df(*dicts):\n",
    "    \"\"\"\n",
    "    将多个评估指标字典合并为一个DataFrame\n",
    "    \n",
    "    参数:\n",
    "    *dicts: 多个包含评估指标的字典\n",
    "    \n",
    "    返回:\n",
    "    merged_df: 合并后的DataFrame\n",
    "    \"\"\"\n",
    "    # 创建空的DataFrame保存所有结果\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    # 创建索引列表\n",
    "    new_index = ['train', 'inter_test', 'henan_test', 'kits_test']\n",
    "    \n",
    "    # 遍历所有字典\n",
    "    for i, d in enumerate(dicts, 1):\n",
    "        # 提取主要指标\n",
    "        metrics = {\n",
    "            'AUC': d['AUC'],\n",
    "            'AUC_CI': f\"({d['AUC_CI'][0]:.3f}-{d['AUC_CI'][1]:.3f})\",\n",
    "            'AUPRC': d['AUPRC'],\n",
    "            'AUPRC_CI': f\"({d['AUPRC_CI'][0]:.3f}-{d['AUPRC_CI'][1]:.3f})\",\n",
    "            'F1': d['F1'],\n",
    "            'F1_CI': f\"({d['F1_CI'][0]:.3f}-{d['F1_CI'][1]:.3f})\",\n",
    "            'ACC': d['ACC'],\n",
    "            'ACC_CI': f\"({d['ACC_CI'][0]:.3f}-{d['ACC_CI'][1]:.3f})\",\n",
    "            'MCC': d['MCC'],\n",
    "            'MCC_CI': f\"({d['MCC_CI'][0]:.3f}-{d['MCC_CI'][1]:.3f})\",\n",
    "            'Precision': d['Precision'],\n",
    "            'Precision_CI': f\"({d['Precision_CI'][0]:.3f}-{d['Precision_CI'][1]:.3f})\",\n",
    "            'Recall': d['Recall'],\n",
    "            'Recall_CI': f\"({d['Recall_CI'][0]:.3f}-{d['Recall_CI'][1]:.3f})\",\n",
    "        }\n",
    "        \n",
    "        # 转换为DataFrame并添加模型标识\n",
    "        df = pd.DataFrame([metrics])\n",
    "        \n",
    "        # 合并到主DataFrame\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # 重排列columns\n",
    "    column_order = ['AUC', 'AUC_CI', 'AUPRC', 'AUPRC_CI', \n",
    "                    'F1', 'F1_CI', 'ACC', 'ACC_CI', 'MCC', 'MCC_CI',\n",
    "                    'Precision', 'Precision_CI', 'Recall', 'Recall_CI']\n",
    "    merged_df = merged_df[column_order]\n",
    "    \n",
    "    # 添加索引列\n",
    "    merged_df.insert(0, 'Dataset', new_index)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing k=20, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing k=20, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=20, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=20, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=21, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=21, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=21, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=21, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=22, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=22, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=22, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=22, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=23, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=23, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=23, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=23, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=24, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=24, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=24, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=24, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=25, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=25, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=25, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=25, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=26, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=26, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=26, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=26, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=27, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=27, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=27, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=27, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=28, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=28, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=28, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=28, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=29, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=29, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=29, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=29, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=30, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=30, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=30, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=30, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=31, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=31, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=31, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=31, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=32, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=32, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=32, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=32, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=33, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=33, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=33, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=33, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=34, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=34, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=34, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=34, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=35, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=35, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=35, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=35, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=36, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=36, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=36, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=36, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=37, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=37, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=37, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=37, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=38, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=38, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=38, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=38, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=39, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=39, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=39, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=39, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=40, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=40, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=40, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=40, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=41, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=41, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=41, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=41, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=42, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=42, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=42, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=42, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=43, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=43, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=43, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=43, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=44, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=44, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=44, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=44, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=45, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=45, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=45, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=45, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=46, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=46, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=46, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=46, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=47, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=47, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=47, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=47, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=48, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=48, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=48, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=48, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=49, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=49, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=49, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=49, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Results saved to 'model_evaluation_results_rad.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_classifiers() -> Dict:\n",
    "    \"\"\"创建分类器字典\"\"\"\n",
    "    return {\n",
    "        'logistic_regression': LogisticRegression(class_weight='balanced'),\n",
    "        'decision_tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "        'random_forest': RandomForestClassifier(class_weight='balanced'),\n",
    "        'svm': SVC(class_weight='balanced', probability=True)\n",
    "    }\n",
    "\n",
    "def create_param_grids() -> Dict:\n",
    "    \"\"\"创建参数网格字典\"\"\"\n",
    "    return {\n",
    "        'logistic_regression': {\n",
    "            'classifier__C': [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "            'classifier__penalty': ['l1', 'l2'],  # 使用l1正则化\n",
    "            'classifier__max_iter': [2000],\n",
    "            'classifier__tol': [1e-4],\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'classifier__criterion': ['gini'],  # gini更常用\n",
    "            'classifier__max_depth': [3, 4, 5, 6],\n",
    "            'classifier__min_samples_split': [4, 5, 6, 7, 8],\n",
    "            'classifier__min_samples_leaf': [1, 2, 3]\n",
    "        },\n",
    "\n",
    "        'random_forest': {\n",
    "            'classifier__n_estimators': [90, 100, 110],         # 围绕100的核心取值\n",
    "            'classifier__max_depth': [4, 5, 6],                 # 中等深度范围，避免过拟合\n",
    "            'classifier__min_samples_split': [5, 6, 7],         # 适中的分裂条件\n",
    "            'classifier__min_samples_leaf': [1, 2],             # 常用的叶节点限制\n",
    "            'classifier__max_features': ['sqrt'],               # sqrt是最常用且效果好的选择\n",
    "            'classifier__max_samples': [0.8, 0.9],              # 主流的采样比例\n",
    "            'classifier__criterion': ['gini']                   # gini更常用且计算效率更高\n",
    "        },\n",
    "        \n",
    "        'svm': {\n",
    "            'classifier__C': [0.01, 0.05, 0.07, 0.1, 0.5, 1.0],  # 扩充正则化参数\n",
    "            'classifier__kernel': ['linear', 'rbf', 'poly'],  # 添加核函数选项\n",
    "            'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1]  # 扩充gamma选项\n",
    "        }\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, X: np.ndarray, y: np.ndarray, dataset_name: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    评估模型性能\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : 训练好的模型\n",
    "    X : array-like\n",
    "        特征矩阵\n",
    "    y : array-like\n",
    "        标签向量\n",
    "    dataset_name : str\n",
    "        数据集名称\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Optional[Dict] : 包含评估指标的字典，如果评估失败则返回None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "        metrics = calculate_metrics(y_proba, y)\n",
    "        metrics['dataset'] = dataset_name\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {dataset_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_grid_search(X_train: np.ndarray, y_train: np.ndarray, \n",
    "                   X_val: np.ndarray, y_val: np.ndarray, \n",
    "                   X_henan: np.ndarray, y_henan: np.ndarray, \n",
    "                   X_kits: np.ndarray, y_kits: np.ndarray, \n",
    "                   selected_classifiers: Optional[List[str]] = None, \n",
    "                   k_range: range = range(20, 50, 1)) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    运行网格搜索并评估模型\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : 训练数据\n",
    "    X_val, y_val : 验证数据\n",
    "    X_henan, y_henan : 河南数据集\n",
    "    X_kits, y_kits : KITS数据集\n",
    "    selected_classifiers : 要评估的分类器列表\n",
    "    k_range : 特征选择的k值范围\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : 评估结果表格\n",
    "    \"\"\"\n",
    "    classifiers = create_classifiers()\n",
    "    param_grids = create_param_grids()\n",
    "    results = []\n",
    "    \n",
    "    if selected_classifiers:\n",
    "        classifiers = {k: v for k, v in classifiers.items() if k in selected_classifiers}\n",
    "    \n",
    "    for k_num in k_range:\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            print(f\"\\nProcessing k={k_num}, classifier={clf_name}\")\n",
    "            \n",
    "            try:\n",
    "                pipeline = Pipeline([\n",
    "                    ('feature_selection', SelectKBest(mutual_info_classif, k=k_num)),\n",
    "                    ('classifier', clf)\n",
    "                ])\n",
    "                \n",
    "                grid_search = GridSearchCV(\n",
    "                    pipeline,\n",
    "                    param_grid=param_grids[clf_name],\n",
    "                    cv=5,\n",
    "                    # scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n",
    "                    refit='roc_auc',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                grid_search.fit(X_train, y_train)\n",
    "                \n",
    "                base_result = {\n",
    "                    'k_num': k_num,\n",
    "                    'classifier': clf_name,\n",
    "                    'best_params': str(grid_search.best_params_),\n",
    "                    'best_score': grid_search.best_score_\n",
    "                }\n",
    "                \n",
    "                datasets = {\n",
    "                    'validation': (X_val, y_val),\n",
    "                    'henan': (X_henan, y_henan),\n",
    "                    'kits': (X_kits, y_kits)\n",
    "                }\n",
    "                \n",
    "                for dataset_name, (X, y) in datasets.items():\n",
    "                    metrics = evaluate_model(grid_search.best_estimator_, X, y, dataset_name)\n",
    "                    if metrics:\n",
    "                        result = base_result.copy()\n",
    "                        result.update(metrics)\n",
    "                        results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {clf_name} with k={k_num}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    results_df.to_excel('model_evaluation_results_rad.xlsx', index=False)\n",
    "    print(\"\\nResults saved to 'model_evaluation_results_rad.xlsx'\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# 使用示例\n",
    "results_df = run_grid_search(X_train, y_train, X_val, y_val, X_henan, y_henan, X_kits, y_kits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': 0.995, 'AUC_CI': (0.9906565656565656, 0.9983706720977598), 'F1': 0.832, 'F1_CI': (0.75, 0.9037037037037037), 'ACC': 0.966, 'ACC_CI': (0.9491833030852994, 0.9818511796733213), 'MCC': 0.815, 'MCC_CI': (0.7320750486967602, 0.8938776287820568), 'AUPRC': 0.957, 'AUPRC_CI': (0.9178303862596339, 0.9859161674451565), 'Precision': 0.887, 'Precision_CI': (0.8035714285714286, 0.9642857142857143), 'Recall': 0.783, 'Recall_CI': (0.6764705882352942, 0.8909090909090909)}\n",
      "{'AUC': 0.787, 'AUC_CI': (0.7020043845912934, 0.8591934381408066), 'F1': 0.205, 'F1_CI': (0.04761904761904763, 0.375), 'ACC': 0.869, 'ACC_CI': (0.8227848101265823, 0.9113924050632911), 'MCC': 0.153, 'MCC_CI': (-0.023433127818198694, 0.3359656089406227), 'AUPRC': 0.325, 'AUPRC_CI': (0.17404024100779836, 0.4901531212407679), 'Precision': 0.308, 'Precision_CI': (0.07692307692307693, 0.5882352941176471), 'Recall': 0.154, 'Recall_CI': (0.03333333333333333, 0.3)}\n",
      "{'AUC': 0.641, 'AUC_CI': (0.5377736663583103, 0.7456395348837209), 'F1': 0.2, 'F1_CI': (0.04651162790697674, 0.3582089552238806), 'ACC': 0.786, 'ACC_CI': (0.7272727272727273, 0.8449197860962567), 'MCC': 0.227, 'MCC_CI': (0.027687476546866516, 0.3909305338754175), 'AUPRC': 0.408, 'AUPRC_CI': (0.2675280538770645, 0.5624666131372933), 'Precision': 0.714, 'Precision_CI': (0.3333333333333333, 1.0), 'Recall': 0.116, 'Recall_CI': (0.02564102564102564, 0.2222222222222222)}\n",
      "{'AUC': 0.636, 'AUC_CI': (0.5287104622871046, 0.7376175548589341), 'F1': 0.154, 'F1_CI': (0.0, 0.3076923076923077), 'ACC': 0.802, 'ACC_CI': (0.7425149700598802, 0.8622754491017964), 'MCC': 0.109, 'MCC_CI': (-0.07715167498104596, 0.309284281491179), 'AUPRC': 0.286, 'AUPRC_CI': (0.1718963732182034, 0.4254865104135666), 'Precision': 0.375, 'Precision_CI': (0.0, 0.7777777777777778), 'Recall': 0.097, 'Recall_CI': (0.0, 0.21875)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "classifier = RandomForestClassifier(\n",
    "n_estimators=110,\n",
    "max_depth=6,\n",
    "min_samples_split=6,\n",
    "min_samples_leaf=1,\n",
    "max_features='sqrt',\n",
    "max_samples=0.8,\n",
    "criterion='gini',\n",
    "random_state=42,  # 添加随机种子以确保可重复性\n",
    "n_jobs=-1,  # 使用所有CPU核心\n",
    "class_weight='balanced'\n",
    ")\n",
    "# classifier = SVC(\n",
    "# C=0.1,\n",
    "# kernel='poly',\n",
    "# gamma=0.01,\n",
    "# probability=True,\n",
    "# class_weight='balanced',\n",
    "# random_state=42\n",
    "# )\n",
    "\n",
    "# 2. 构建pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectKBest(mutual_info_classif, k=43)),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# 3. 训练模型\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. 在训练集和内部验证集上评估\n",
    "train_proba = pipeline.predict_proba(X_train)[:, 1]\n",
    "val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 5. 在两个测试集上评估\n",
    "henan_proba = pipeline.predict_proba(X_henan)[:, 1]\n",
    "kits_proba = pipeline.predict_proba(X_kits)[:, 1]\n",
    "\n",
    "# 将计算指标转为表格\n",
    "train_df = pd.DataFrame({\n",
    "    'dataset': 'train',\n",
    "    'name': X_train.index,\n",
    "    'ground_truth': y_train,\n",
    "    'probability': train_proba\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'dataset': 'val', \n",
    "    'name': X_val.index,\n",
    "    'ground_truth': y_val,\n",
    "    'probability': val_proba\n",
    "})\n",
    "\n",
    "henan_df = pd.DataFrame({\n",
    "    'dataset': 'henan',\n",
    "    'name': X_henan.index,\n",
    "    'ground_truth': y_henan,\n",
    "    'probability': henan_proba\n",
    "})\n",
    "\n",
    "kits_df = pd.DataFrame({\n",
    "    'dataset': 'kits',\n",
    "    'name': X_kits.index,\n",
    "    'ground_truth': y_kits,\n",
    "    'probability': kits_proba\n",
    "})\n",
    "# 合并所有DataFrame\n",
    "final_df = pd.concat([train_df, val_df, henan_df, kits_df], ignore_index=True)\n",
    "# 保存为Excel文件\n",
    "final_df.to_excel('rad.xlsx', index=False)\n",
    "\n",
    "# 6. 计算各项指标\n",
    "result_train = calculate_metrics(train_proba, y_train)\n",
    "result_val = calculate_metrics(val_proba, y_val)\n",
    "result_henan = calculate_metrics(henan_proba, y_henan)\n",
    "result_kits = calculate_metrics(kits_proba, y_kits)\n",
    "print(result_train)\n",
    "print(result_val)\n",
    "print(result_henan)\n",
    "print(result_kits)\n",
    "\n",
    "# 合并字典\n",
    "merged_results = merge_dicts_to_df(result_train, result_val, result_henan, result_kits)\n",
    "\n",
    "# 保存为CSV文件\n",
    "merged_results.to_excel('results_rad.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': 0.899, 'AUC_CI': (0.862, 0.935), 'F1': 0.376, 'F1_CI': (0.312, 0.443), 'ACC': 0.644, 'ACC_CI': (0.606, 0.684), 'MCC': 0.353, 'MCC_CI': (0.296, 0.408), 'AUPRC': 0.615, 'AUPRC_CI': (0.488, 0.731), 'Precision': 0.234, 'Precision_CI': (0.183, 0.288), 'Recall': 0.952, 'Recall_CI': (0.892, 1.0)}\n",
      "{'AUC': 0.75, 'AUC_CI': (0.648, 0.839), 'F1': 0.261, 'F1_CI': (0.167, 0.359), 'ACC': 0.57, 'ACC_CI': (0.506, 0.633), 'MCC': 0.181, 'MCC_CI': (0.064, 0.286), 'AUPRC': 0.257, 'AUPRC_CI': (0.142, 0.45), 'Precision': 0.158, 'Precision_CI': (0.096, 0.225), 'Recall': 0.75, 'Recall_CI': (0.565, 0.92)}\n",
      "{'AUC': 0.564, 'AUC_CI': (0.471, 0.659), 'F1': 0.324, 'F1_CI': (0.222, 0.429), 'ACC': 0.508, 'ACC_CI': (0.439, 0.578), 'MCC': 0.016, 'MCC_CI': (-0.127, 0.157), 'AUPRC': 0.305, 'AUPRC_CI': (0.19, 0.432), 'Precision': 0.237, 'Precision_CI': (0.152, 0.33), 'Recall': 0.512, 'Recall_CI': (0.37, 0.667)}\n",
      "{'AUC': 0.68, 'AUC_CI': (0.558, 0.789), 'F1': 0.404, 'F1_CI': (0.283, 0.509), 'ACC': 0.593, 'ACC_CI': (0.521, 0.665), 'MCC': 0.234, 'MCC_CI': (0.082, 0.369), 'AUPRC': 0.305, 'AUPRC_CI': (0.187, 0.458), 'Precision': 0.277, 'Precision_CI': (0.188, 0.383), 'Recall': 0.742, 'Recall_CI': (0.581, 0.889)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# 创建完整的管道\n",
    "def create_training_pipeline():\n",
    "    # 定义基础分类器\n",
    "    base_classifier = RandomForestClassifier(\n",
    "        n_estimators=110,\n",
    "        max_depth=6,\n",
    "        min_samples_split=6,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        max_samples=0.8,\n",
    "        criterion='gini',\n",
    "        random_state=42,  # 添加随机种子以确保可重复性\n",
    "        n_jobs=-1,  # 使用所有CPU核心\n",
    "        class_weight='balanced')\n",
    "\n",
    "    # 创建完整的预处理和训练管道\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest(mutual_info_classif, k=4)),\n",
    "        # ('smote', SMOTE(sampling_strategy='auto',random_state=42)),\n",
    "        ('undersampler', RandomUnderSampler(sampling_strategy='auto', random_state=42)),\n",
    "        ('classifier', base_classifier)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# 创建管道\n",
    "pipeline = create_training_pipeline()\n",
    "\n",
    "# 训练模型\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 获取预测概率\n",
    "train_proba = pipeline.predict_proba(X_train)[:, 1]\n",
    "val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "henan_proba = pipeline.predict_proba(X_henan)[:, 1]\n",
    "kits_proba = pipeline.predict_proba(X_kits)[:, 1]\n",
    "\n",
    "# 计算各项指标\n",
    "result_train = calculate_metrics(train_proba, y_train)\n",
    "result_val = calculate_metrics(val_proba, y_val)\n",
    "result_henan = calculate_metrics(henan_proba, y_henan)\n",
    "result_kits = calculate_metrics(kits_proba, y_kits)\n",
    "print(result_train)\n",
    "print(result_val)\n",
    "print(result_henan)\n",
    "print(result_kits)\n",
    "\n",
    "# 合并字典\n",
    "merged_results = merge_dicts_to_df(result_train, result_val, result_henan, result_kits)\n",
    "\n",
    "# 保存为CSV文件\n",
    "merged_results.to_excel('results_rad.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图像和数据已保存到目录: feature_importance_results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shap\n",
    "def save_feature_importance_plots(pipeline, X_train, y_train, X_val, y_val, feature_names, save_dir='feature_importance_plots'):\n",
    "    \"\"\"\n",
    "    保存所有特征重要性相关的图为PDF\n",
    "    \"\"\"\n",
    "    # 创建保存目录\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    # 1. 特征选择分析图\n",
    "    selector = pipeline.named_steps['feature_selection']\n",
    "    selected_mask = selector.get_support()\n",
    "    selected_features = feature_names[selected_mask]\n",
    "    scores = selector.scores_\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(selected_features)), \n",
    "            pd.DataFrame({'Feature': feature_names, 'Selected': selected_mask, \n",
    "                         'Mutual_Info_Score': scores})[selected_mask]['Mutual_Info_Score'])\n",
    "    plt.xticks(range(len(selected_features)), selected_features, rotation=45, ha='right')\n",
    "    plt.title('Mutual Information Scores of Selected Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'mutual_info_scores.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Random Forest特征重要性图\n",
    "    rf_model = pipeline.named_steps['classifier']\n",
    "    importances = rf_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': importances,\n",
    "        'Std': std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    feature_importance.plot(kind='bar', x='Feature', y='Importance', yerr='Std',\n",
    "                          capsize=5, alpha=0.8)\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'rf_importance.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. SHAP值分析图\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_selected = X_train.iloc[:, selected_mask].values\n",
    "    else:\n",
    "        X_selected = X_train[:, selected_mask]\n",
    "    \n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    shap_values = explainer.shap_values(X_selected)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # SHAP summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_selected, \n",
    "                     feature_names=selected_features.tolist(),\n",
    "                     show=False)\n",
    "    plt.title('SHAP Summary Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'shap_summary.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # SHAP依赖图（top 3特征）\n",
    "    mean_abs_shap = np.abs(shap_values).mean(0)\n",
    "    top_features_idx = np.argsort(mean_abs_shap)[-3:]\n",
    "    \n",
    "    for idx in top_features_idx:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        shap.dependence_plot(idx, shap_values, X_selected,\n",
    "                           feature_names=selected_features.tolist(),\n",
    "                           show=False)\n",
    "        plt.title(f'SHAP Dependence Plot - {selected_features[idx]}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f'shap_dependence_{selected_features[idx]}.pdf'))\n",
    "        plt.close()\n",
    "        \n",
    "    # 4. 合并所有重要性指标并保存\n",
    "    final_importance = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Mutual_Info_Score': scores[selected_mask],\n",
    "        'RF_Importance': importances,\n",
    "        'RF_Importance_Std': std,\n",
    "        'Mean_Abs_SHAP': mean_abs_shap\n",
    "    })\n",
    "    \n",
    "    final_importance.to_csv(os.path.join(save_dir, 'feature_importance_metrics.csv'), index=False)\n",
    "    \n",
    "    print(f\"所有图像和数据已保存到目录: {save_dir}\")\n",
    "    return final_importance\n",
    "\n",
    "# 使用示例:\n",
    "results = save_feature_importance_plots(\n",
    "    pipeline=pipeline,\n",
    "    X_train=X_train, \n",
    "    y_train=y_train,\n",
    "    X_val=X_val, \n",
    "    y_val=y_val,\n",
    "    feature_names=np.array(X_train.columns),\n",
    "    save_dir='feature_importance_results'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
