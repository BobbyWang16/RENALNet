{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "# 对比四个简单的分类器\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# select k best features\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# import models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAI JING JIAN' 'CAO XUE LIN' 'CHEN AI ZHEN' 'CHEN CUI YING'\n",
      " 'CHEN GUO HUA' 'CHEN JI MING' 'CHEN JIA YOU' 'CHEN JINHUA'\n",
      " 'CHEN LIAN YING' 'CHEN NAN ZHEN' 'CHEN QI XUE' 'CHEN QIONG'\n",
      " 'CHEN SAN QING' 'CHEN XIAN CI' 'CHEN XIAO XUN' 'CHEN XIAO'\n",
      " 'CHEN XIN JIANG' 'CHEN XU LANG' 'CHEN YAN WEN' 'CHEN YANG GUI'\n",
      " 'CHENG BO JUN' 'CHENG CHAO HAI' 'CHENG SHE BAO' 'CUI XIAO XIN'\n",
      " 'DAN BI ZHONG' 'DENG CHAO JU' 'DENG XUAN YONG' 'DING YOU QING' 'DONG XUE'\n",
      " 'DU LIANG QIAO' 'FAN XIN NU' 'FAN YAN XIA' 'FANG XI ZHI' 'FEI JIANG PING'\n",
      " 'FENG DE YIN' 'FENG KAI QUAN' 'FU KAI' 'FU XIAO LAN' 'FU XIONG CAI'\n",
      " 'FU YI GUO' 'GAO GUO YING' 'GAO HUI PIN' 'GONG GUO BING' 'GONG LI HUA'\n",
      " 'GONG SHENG YONG' 'GONG YONG ZHI' 'GU PAN' 'GU YING' 'GUO DING GEN'\n",
      " 'GUO WEI' 'HAN LI YING' 'HE AI QIN' 'HE WEN HAO' 'HE WEN HUA'\n",
      " 'HU CHANG HUA' 'HU DONG E' 'HU HUI RAN' 'HU KUN YI' 'HU QUN XI'\n",
      " 'HU SHENG MING' 'HU WAN CHANG' 'HU WEN YUE' 'HU YUN HONG' 'HU ZHEN PING'\n",
      " 'HUA LAN' 'HUANG HUI_M' 'HUANG JIAO XIA' 'HUANG MING XUE' 'HUANG PING'\n",
      " 'HUANG WEI GONG' 'JIA FENG JIE' 'JIANG YU XIA' 'JIANG ZHONG HAO'\n",
      " 'JIN HAI YI' 'JIN YUE MING' 'JING XING HUA' 'LEI XIAN YONG'\n",
      " 'LENG XIAO YU' 'LI AN RONG' 'LI DING CHAO' 'LI DONG AI' 'LI HAI CANG'\n",
      " 'LI HE RONG' 'LI JIA PING' 'LI JIAN QUN' 'LI JIAO ZHI1' 'LI JIN HUA'\n",
      " 'LI JUN HUA' 'LI KAI RONG' 'LI QING NAN' 'LI WEI' 'LI XIAO QIN'\n",
      " 'LI XU LIN' 'LIAN ZHEN HUA' 'LIN KE QING' 'LIN RUI LAN' 'LIU FENG'\n",
      " 'LIU GUANG QUAN' 'LIU HE QING' 'LIU HONG ZHI' 'LIU JIN XIA' 'LIU JIN YAN'\n",
      " 'LIU JIN' 'LIU JUN ZHEN' 'LIU LIAN XI' 'LIU MING FENG' 'LIU PING FU'\n",
      " 'LIU SAN LIN' 'LIU SHAO QUAN' 'LIU SHI YING' 'LIU SU FEN' 'LIU WEI FANG'\n",
      " 'LIU WEI MAO' 'LIU YIN GUI' 'LIU YING ZHONG' 'LIU YONG' 'LIU YU'\n",
      " 'LIU ZUAN GE' 'LU FANG' 'LU MING ZHONG' 'LU XIU YUE' 'LU ZHONG SHENG'\n",
      " 'LU ZU XIANG' 'LUO HONG LI' 'LV HOU YOU' 'LV ZHI MIN' 'MA BO LIN'\n",
      " 'MA FENG' 'MA HONG WEI' 'MA JIANGUO' 'MA XIAO ZHEN' 'MAO GUI FANG'\n",
      " 'PENG DAI XI' 'PENG GUANG ZU' 'PENG GUO FA' 'PENG LU' 'PENG WEI PING'\n",
      " 'PENG XIAO YING' 'QIU FENG' 'QIU XI PING' 'QU XIANG BING' 'RONG ZONG AN'\n",
      " 'SHAO BIN' 'SHAO FENG YING' 'SHENG ZHOU' 'SHI JIAN GANG' 'SHI LI XIN'\n",
      " 'SHI RUI' 'SHU HONG FEI' 'SONG GUANG JIE' 'SONG JIAN HUA' 'SONG YI'\n",
      " 'SU GUANG HAI' 'SUN CHANG LIAN' 'SUN GUI BIN' 'SUN HUI FENG'\n",
      " 'SUN JIN XIANG' 'SUN SAN JUN' 'TANG GUANG HUA' 'TAO CHAO YU'\n",
      " 'TAO GUI YING' 'TAO JIAN FANG' 'TIAN JIAO ZHI' 'TIAN MING ZHI' 'TJH0002'\n",
      " 'TJH0003' 'TJH0010' 'TJH0016' 'TJH0020' 'TJH0024' 'TJH0026' 'TJH0028'\n",
      " 'TJH0030' 'TJH0032' 'TJH0035' 'TJH0041' 'TJH0042' 'TJH0043' 'TJH0044'\n",
      " 'TJH0045' 'TJH0047' 'TJH0048' 'TJH0049' 'TJH0051' 'TJH0053' 'TJH0054'\n",
      " 'TJH0057' 'TJH0063' 'TJH0067' 'TJH0069' 'TJH0072' 'TJH0074' 'TJH0075'\n",
      " 'TJH0076' 'TJH0077' 'TJH0080' 'TJH0081' 'TJH0085' 'TJH0091' 'TJH0098'\n",
      " 'TJH0100' 'TJH0101' 'TJH0102' 'TJH0108' 'TJH0113' 'TJH0123' 'TJH0126'\n",
      " 'TJH0131' 'TJH0132' 'TJH0134' 'TJH0137' 'TJH0139' 'TJH0142' 'TJH0148'\n",
      " 'TJH0149' 'TJH0151' 'TJH0154' 'TJH0157' 'TJH0162' 'TJH0167' 'TJH0170'\n",
      " 'TJH0179' 'TJH0181' 'TJH0183' 'TJH0188' 'TJH0189' 'TJH0190' 'TJH0191'\n",
      " 'TJH0196' 'TJH0203' 'TJH0206' 'TJH0209' 'TJH0217' 'TJH0221' 'TJH0227'\n",
      " 'TJH0229' 'TJH0231' 'TJH0232' 'TJH0233' 'TJH0240' 'TJH0243' 'TJH0245'\n",
      " 'TJH0248' 'TJH0250' 'TJH0254' 'TJH0258' 'TJH0261' 'TJH0262' 'TJH0266'\n",
      " 'TJH0272' 'TJH0275' 'TJH0278' 'TJH0280' 'TJH0281' 'TJH0282' 'TJH0283'\n",
      " 'TJH0290' 'TJH0298' 'TJH0300' 'TJH0309' 'TJH0310' 'TJH0312' 'TJH0313'\n",
      " 'TJH0317' 'TJH0319' 'TJH0320' 'TJH0321' 'TJH0326' 'TJH0330' 'TJH0332'\n",
      " 'TJH0335' 'TJH0336' 'TJH0339' 'TJH0340' 'TJH0342' 'TJH0344' 'TJH0346'\n",
      " 'TJH0347' 'TJH0357' 'TJH0360' 'TJH0367' 'TJH0386' 'TJH0392' 'TJH0396'\n",
      " 'TJH0397' 'TJH0398' 'TJH0399' 'TJH0402' 'TJH0404' 'TJH0442' 'TJH0443'\n",
      " 'TJH0445' 'TJH0464' 'TJH0465' 'TJH0468' 'TJH0469' 'TJH0471' 'TJH0475'\n",
      " 'TJH0477' 'TJH0486' 'TJH0489' 'TJH0494' 'TJH0498' 'TJH0501' 'TJH0503'\n",
      " 'TJH0505' 'TJH0506' 'TJH0507' 'TJH0514' 'TJH0518' 'TJH0527' 'TJH0529'\n",
      " 'TJH0537' 'TJH0538' 'TJH0543' 'TJH0544' 'TJH0547' 'TJH0551' 'TJH0553'\n",
      " 'TJH0554' 'TJH0574' 'TJH0575' 'TJH0579' 'TJH0590' 'TJH0593' 'TJH0598'\n",
      " 'TJH0602' 'TJH0604' 'TJH0606' 'TJH0618' 'TJH0620' 'TJH0622' 'TJH0626'\n",
      " 'TJH0627' 'TJH0632' 'TJH0634' 'TJH0637' 'TJH0641' 'TJH0642' 'TJH0644'\n",
      " 'TJH0654' 'TJH0658' 'TJH0663' 'TJH0665' 'TJH0669' 'TJH0672' 'TJH0675'\n",
      " 'TJH0679' 'TJH0680' 'TJH0682' 'TJH0684' 'TJH0690' 'TJH0691' 'TJH0692'\n",
      " 'TJH0695' 'TJH0699' 'TJH0701' 'TJH0705' 'TJH0706' 'TJH0714' 'TJH0715'\n",
      " 'TJH0724' 'TJH0727' 'TJH0729' 'TJH0730' 'TJH0733' 'TJH0735' 'TJH0741'\n",
      " 'TJH0749' 'TJH0753' 'TJH0758' 'TJH0761' 'TJH0767' 'TJH0768' 'TJH0769'\n",
      " 'TJH0773' 'TJH0776' 'TJH0777' 'TJH0782' 'TJH0786' 'TJH0788' 'TJH0790'\n",
      " 'TJH0792' 'TJH0796' 'TJH0799' 'TJH0804' 'TJH0810' 'TJH0811' 'TJH0816'\n",
      " 'TJH0819' 'TJH0822' 'TJH0825' 'TJH0827' 'TJH0828' 'TJH0832' 'TJH0834'\n",
      " 'TJH0838' 'TJH0839' 'TJH0845' 'TJH0847' 'TJH0848' 'TJH0850' 'TJH0856'\n",
      " 'TJH0859' 'TJH0860' 'TJH0864' 'TJH0865' 'TJH0868' 'TJH0874' 'TU WEI'\n",
      " 'WAN WEN LAN' 'WANG BAI RU' 'WANG DONG MING' 'WANG HAO' 'WANG HUA YING'\n",
      " 'WANG JIN BO' 'WANG JING' 'WANG LIN' 'WANG MEI PENG' 'WANG MU LAN'\n",
      " 'WANG RUN HUA' 'WANG SHAN HAI' 'WANG SHENG HUA' 'WANG SONG QIU'\n",
      " 'WANG SUI DANG' 'WANG TIAN HUA' 'WANG XING XIANG' 'WANG YAN MEI'\n",
      " 'WANG YAN PU' 'WANG YAN' 'WANG YI CAN' 'WANG YIN XIANG' 'WANG YING ZHEN'\n",
      " 'WANG YUN XIU' 'WANG ZHI CAI' 'WANG ZU WEI' 'WEI CHEN XI' 'WU DONG E'\n",
      " 'WU FENG' 'WU FU LAN' 'WU HAO' 'WU HUI XIANG' 'WU JU RONG' 'WU KE RONG'\n",
      " 'WU LI ZHI' 'WU SONG NIAN' 'WU XIE CHENG' 'WU XIN JIE' 'WU YOU AI'\n",
      " 'WU ZHONG GUO' 'XIA GUO HUA' 'XIA QI LIN' 'XIANG SHENG QIAN'\n",
      " 'XIAO XIN NIAN' 'XIAO ZHEN YING' 'XIAO ZHI PING' 'XIE GUANG BAO'\n",
      " 'XIE HONG' 'XIONG GUO CHAO' 'XIONG HAN YING' 'XIONG HONG GUANG'\n",
      " 'XIONG HOU PING' 'XIONG LAI YI' 'XIONG TIAN HUA' 'XIONG YI PING'\n",
      " 'XIONG ZHONG QIN' 'XU AI LING' 'XU BAO JIAN' 'XU CHANG FU' 'XU CUI HUA'\n",
      " 'XU DE WEN' 'XU FU ZHEN' 'XU GUANG FU' 'XU HONG WEI' 'XU JING'\n",
      " 'XU REN DOU' 'XU XI GUI' 'XU XUE TAO' 'XU YUE GANG' 'XU ZE WEN'\n",
      " 'XUE HONG' 'YAN GUI JU' 'YAN YOU FU' 'YANG BIN BIN' 'YANG FU HUA'\n",
      " 'YANG GEN' 'YANG GUANGMING' 'YANG HAO' 'YANG JING' 'YANG XU' 'YANG YAN'\n",
      " 'YAO CHENG JUN' 'YAO SHU FANG' 'YE SHUILI' 'YE YI BING' 'YI WEN HUA'\n",
      " 'YOU XING MEI' 'YU DING WU' 'YU JI FA' 'YU RUN SHENG' 'YU TAO XIANG'\n",
      " 'YU YING MING' 'YU YOU JUN' 'YU ZHI BIN' 'YUAN JIN CHENG' 'YUAN YU QIN'\n",
      " 'YUAN ZHENGZHONG' 'ZHANG BEN SHAN' 'ZHANG DE JUN' 'ZHANG GUANG HUA'\n",
      " 'ZHANG HAN ZHEN' 'ZHANG JIA SHUN' 'ZHANG JIAN' 'ZHANG KUI MING'\n",
      " 'ZHANG LA XIANG' 'ZHANG MEI XIU' 'ZHANG QI YING' 'ZHANG SHUN GUO'\n",
      " 'ZHANG TAO' 'ZHANG TIAN MING' 'ZHANG TING SHENG' 'ZHANG TING'\n",
      " 'ZHANG YAN LING' 'ZHANG YING' 'ZHANG YONG HUA' 'ZHANG YUAN YUAN'\n",
      " 'ZHANG YUN PING' 'ZHANG ZHONG' 'ZHANG ZI LI' 'ZHAO BAO LIN'\n",
      " 'ZHAO CHAO QI' 'ZHAO KUN XIU' 'ZHAO PENG' 'ZHAO XIAO BING' 'ZHENG AN BAO'\n",
      " 'ZHENG SHENG HONG' 'ZHENG YA XUE' 'ZHENG ZHEN PING' 'ZHOU CHI PING'\n",
      " 'ZHOU WEI' 'ZHOU XIAN FA' 'ZHOU XIAO BING' 'ZHOU YAN' 'ZHU BANG JUN'\n",
      " 'ZHU CENG HAN' 'ZHU HONG' 'ZHU XIU JIANG' 'ZHU XIU QING' 'ZOU FANG BEN'\n",
      " 'ZOU QIN' 'ZOU SHI YAO']\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data_label = pd.read_excel(\"./feature/class.xlsx\", index_col=1)\n",
    "data_label = data_label[data_label['exclusion'] == 1]\n",
    "# index去掉.npy\n",
    "data_label.index = data_label.index.str.replace('.npy', '')\n",
    "\n",
    "tongji_label = data_label[data_label['dataset'] == 'tongji'][\"class\"]\n",
    "xiangyang_label = data_label[data_label['dataset'] == 'xiangyang'][\"class\"]\n",
    "kits_label = data_label[data_label['dataset'] == 'kits'][\"class\"]\n",
    "henan_label = data_label[data_label['dataset'] == 'henan'][\"class\"]\n",
    "\n",
    "tongji_rad = pd.read_excel('./feature/rad/rad_tongji.xlsx', index_col=0).iloc[:, 37:]\n",
    "xiangyang_rad = pd.read_excel('./feature/rad/rad_xiangyang.xlsx', index_col=0).iloc[:, 37:]\n",
    "kits_rad = pd.read_excel('./feature/rad/rad_kits.xlsx', index_col=0).iloc[:, 37:]\n",
    "henan_rad = pd.read_excel('./feature/rad/rad_henan.xlsx', index_col=0).iloc[:, 37:]\n",
    "# 在column名前添加rad_\n",
    "tongji_rad.columns = ['rad_' + str(i) for i in tongji_rad.columns]\n",
    "xiangyang_rad.columns = ['rad_' + str(i) for i in xiangyang_rad.columns]\n",
    "kits_rad.columns = ['rad_' + str(i) for i in kits_rad.columns]\n",
    "henan_rad.columns = ['rad_' + str(i) for i in henan_rad.columns]\n",
    "\n",
    "tongji_ring = pd.read_excel('./feature/ring/ring_tongji.xlsx', index_col=0).iloc[:, 37:]\n",
    "xiangyang_ring = pd.read_excel('./feature/ring/ring_xiangyang.xlsx', index_col=0).iloc[:, 22:]\n",
    "kits_ring = pd.read_excel('./feature/ring/ring_kits.xlsx', index_col=0).iloc[:, 22:]\n",
    "henan_ring = pd.read_excel('./feature/ring/ring_henan.xlsx', index_col=0).iloc[:, 22:]\n",
    "# 在column名前添加ring_\n",
    "tongji_ring.columns = ['ring_' + str(i) for i in tongji_ring.columns]\n",
    "xiangyang_ring.columns = ['ring_' + str(i) for i in xiangyang_ring.columns]\n",
    "kits_ring.columns = ['ring_' + str(i) for i in kits_ring.columns]\n",
    "henan_ring.columns = ['ring_' + str(i) for i in henan_ring.columns]\n",
    "\n",
    "# 合并ring到rad\n",
    "tongji_rad = pd.concat([tongji_rad, tongji_ring], axis=1)\n",
    "xiangyang_rad = pd.concat([xiangyang_rad, xiangyang_ring], axis=1)\n",
    "kits_rad = pd.concat([kits_rad, kits_ring], axis=1)\n",
    "henan_rad = pd.concat([henan_rad, henan_ring], axis=1)\n",
    "\n",
    "# 合并数据表格\n",
    "feature_rad = pd.concat([tongji_rad, xiangyang_rad], axis=0)\n",
    "label = pd.concat([tongji_label, xiangyang_label], axis=0)\n",
    "\n",
    "# 按照index排序\n",
    "feature_rad = feature_rad.sort_index()\n",
    "label = label.sort_index()\n",
    "\n",
    "kits_rad = kits_rad.sort_index()\n",
    "kits_label = kits_label.sort_index()\n",
    "\n",
    "henan_rad = henan_rad.sort_index()\n",
    "henan_label = henan_label.sort_index()\n",
    "\n",
    "assert all(feature_rad.index == label.index)\n",
    "assert all(kits_rad.index == kits_label.index)\n",
    "assert all(henan_rad.index == henan_label.index)\n",
    "\n",
    "train_test_index = pd.read_excel(\"./index.xlsx\", index_col=0)\n",
    "train_index = train_test_index[train_test_index.index == 'train']['name'].values.ravel()\n",
    "val_index = train_test_index[train_test_index.index == 'val']['name'].values.ravel()\n",
    "X_train = feature_rad.loc[train_index]\n",
    "y_train = label.loc[train_index]\n",
    "X_val = feature_rad.loc[val_index]\n",
    "y_val = label.loc[val_index]\n",
    "\n",
    "# # 定义 X_train, y_train, X_val, y_val\n",
    "# X_train, X_val, y_train, y_val = train_test_split(feature_rad, label, test_size=0.3, random_state=42)\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "X_val = pd.DataFrame(scaler.transform(X_val), index=X_val.index, columns=X_val.columns)\n",
    "X_henan = pd.DataFrame(scaler.transform(henan_rad), index=henan_rad.index, columns=henan_rad.columns)\n",
    "X_kits = pd.DataFrame(scaler.transform(kits_rad), index=kits_rad.index, columns=kits_rad.columns)\n",
    "y_henan = henan_label\n",
    "y_kits = kits_label\n",
    "assert all(X_kits.index == y_kits.index)\n",
    "assert all(X_henan.index == y_henan.index)\n",
    "y_train = y_train.values.ravel()\n",
    "y_val = y_val.values.ravel()\n",
    "y_henan = y_henan.values.ravel()\n",
    "y_kits = y_kits.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(metric_func, y_prob, y_true, n_iterations=1000, ci=0.95):\n",
    "    \"\"\"\n",
    "    计算bootstrap置信区间\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    size = len(y_true)\n",
    "    scores = []\n",
    "    \n",
    "    rng = np.random.RandomState(42)  # 设置随机种子\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # 随机抽样（使用替换）\n",
    "        indices = rng.randint(0, size, size=size)\n",
    "        score = metric_func(y_true[indices], y_prob[indices])\n",
    "        scores.append(score)\n",
    "    \n",
    "    # 计算置信区间\n",
    "    sorted_scores = np.sort(scores)\n",
    "    alpha = (1 - ci) / 2\n",
    "    lower_bound = sorted_scores[int(alpha * n_iterations)]\n",
    "    upper_bound = sorted_scores[int((1 - alpha) * n_iterations)]\n",
    "    \n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "\n",
    "# def bootstrap_ci(metric_func, y_prob, y_true, n_iterations=1000, ci=0.95):\n",
    "#     \"\"\"\n",
    "#     使用bootstrap方法计算指标的置信区间\n",
    "#     \"\"\"\n",
    "#     scores = []\n",
    "#     size = len(y_true)\n",
    "    \n",
    "#     for _ in range(n_iterations):\n",
    "#         # 随机抽样\n",
    "#         indices = np.random.randint(0, size, size)\n",
    "#         score = metric_func(y_true[indices], y_prob[indices])\n",
    "#         scores.append(score)\n",
    "    \n",
    "#     # 计算置信区间\n",
    "#     lower = np.percentile(scores, ((1-ci)/2)*100)\n",
    "#     upper = np.percentile(scores, (1-(1-ci)/2)*100)\n",
    "    \n",
    "#     return round(lower, 3), round(upper, 3)\n",
    "\n",
    "def calculate_metrics(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    计算不平衡二分类问题的评价指标，包括95%置信区间\n",
    "    \n",
    "    参数：\n",
    "    y_prob: numpy array, 预测为正类(少数类)的概率值\n",
    "    y_true: numpy array, 真实标签 (0为多数类，1为少数类)\n",
    "    \n",
    "    返回：\n",
    "    dict: 包含评价指标和置信区间的字典\n",
    "    \"\"\"\n",
    "    # 1. AUC\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    auc_ci = bootstrap_ci(roc_auc_score, y_prob, y_true)\n",
    "    \n",
    "    # 2. 通过优化F1-score选择最佳阈值\n",
    "    # precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    # f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "    # f1_scores = np.nan_to_num(f1_scores)\n",
    "    # best_threshold = thresholds[np.argmax(f1_scores[:-1])]\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    # 使用最佳阈值进行预测\n",
    "    y_pred = (y_prob >= best_threshold).astype(int)\n",
    "    \n",
    "    # 3. F1-score\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    f1_ci = bootstrap_ci(lambda y_t, y_p: f1_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                        y_prob, y_true)\n",
    "    \n",
    "    # 4. ACC\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    acc_ci = bootstrap_ci(lambda y_t, y_p: accuracy_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                         y_prob, y_true)\n",
    "    \n",
    "    # 5. MCC\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    mcc_ci = bootstrap_ci(lambda y_t, y_p: matthews_corrcoef(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                         y_prob, y_true)\n",
    "    \n",
    "    # 6. AUPRC\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    def auprc_score(y_t, y_p):\n",
    "        p, r, _ = precision_recall_curve(y_t, y_p)\n",
    "        return auc(r, p)\n",
    "    \n",
    "    auprc_ci = bootstrap_ci(auprc_score, y_prob, y_true)\n",
    "    \n",
    "    # 7. Precision\n",
    "    precisionscore = precision_score(y_true, y_pred)\n",
    "    precision_ci = bootstrap_ci(lambda y_t, y_p: precision_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                              y_prob, y_true)\n",
    "\n",
    "    # 8. Recall\n",
    "    recallscore = recall_score(y_true, y_pred)\n",
    "    recall_ci = bootstrap_ci(lambda y_t, y_p: recall_score(y_t, (y_p >= best_threshold).astype(int)),\n",
    "                            y_prob, y_true)\n",
    "\n",
    "    metrics = {\n",
    "        'AUC': round(auc_score, 3),\n",
    "        'AUC_CI': auc_ci,\n",
    "        'F1': round(f1, 3),\n",
    "        'F1_CI': f1_ci,\n",
    "        'ACC': round(acc, 3),\n",
    "        'ACC_CI': acc_ci,\n",
    "        'MCC': round(mcc, 3),\n",
    "        'MCC_CI': mcc_ci,\n",
    "        'AUPRC': round(auprc, 3),\n",
    "        'AUPRC_CI': auprc_ci,\n",
    "        # 'threshold': round(best_threshold, 3),\n",
    "        'Precision': round(precisionscore, 3),\n",
    "        'Precision_CI': precision_ci,\n",
    "        'Recall': round(recallscore, 3),\n",
    "        'Recall_CI': recall_ci\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def format_metrics(metrics):\n",
    "    \"\"\"\n",
    "    格式化指标输出，包含置信区间\n",
    "    \"\"\"\n",
    "    formatted = {}\n",
    "    for key in metrics:\n",
    "        if key.endswith('_CI'):\n",
    "            continue\n",
    "        if key + '_CI' in metrics:\n",
    "            formatted[key] = f\"{metrics[key]} ({metrics[key+'_CI'][0]}-{metrics[key+'_CI'][1]})\"\n",
    "        else:\n",
    "            formatted[key] = f\"{metrics[key]}\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "def merge_dicts_to_df(*dicts):\n",
    "    \"\"\"\n",
    "    将多个评估指标字典合并为一个DataFrame\n",
    "    \n",
    "    参数:\n",
    "    *dicts: 多个包含评估指标的字典\n",
    "    \n",
    "    返回:\n",
    "    merged_df: 合并后的DataFrame\n",
    "    \"\"\"\n",
    "    # 创建空的DataFrame保存所有结果\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    # 创建索引列表\n",
    "    new_index = ['train', 'inter_test', 'henan_test', 'kits_test']\n",
    "    \n",
    "    # 遍历所有字典\n",
    "    for i, d in enumerate(dicts, 1):\n",
    "        # 提取主要指标\n",
    "        metrics = {\n",
    "            'AUC': d['AUC'],\n",
    "            'AUC_CI': f\"({d['AUC_CI'][0]:.3f}-{d['AUC_CI'][1]:.3f})\",\n",
    "            'AUPRC': d['AUPRC'],\n",
    "            'AUPRC_CI': f\"({d['AUPRC_CI'][0]:.3f}-{d['AUPRC_CI'][1]:.3f})\",\n",
    "            'F1': d['F1'],\n",
    "            'F1_CI': f\"({d['F1_CI'][0]:.3f}-{d['F1_CI'][1]:.3f})\",\n",
    "            'ACC': d['ACC'],\n",
    "            'ACC_CI': f\"({d['ACC_CI'][0]:.3f}-{d['ACC_CI'][1]:.3f})\",\n",
    "            'MCC': d['MCC'],\n",
    "            'MCC_CI': f\"({d['MCC_CI'][0]:.3f}-{d['MCC_CI'][1]:.3f})\",\n",
    "            'Precision': d['Precision'],\n",
    "            'Precision_CI': f\"({d['Precision_CI'][0]:.3f}-{d['Precision_CI'][1]:.3f})\",\n",
    "            'Recall': d['Recall'],\n",
    "            'Recall_CI': f\"({d['Recall_CI'][0]:.3f}-{d['Recall_CI'][1]:.3f})\",\n",
    "        }\n",
    "        \n",
    "        # 转换为DataFrame并添加模型标识\n",
    "        df = pd.DataFrame([metrics])\n",
    "        \n",
    "        # 合并到主DataFrame\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # 重排列columns\n",
    "    column_order = ['AUC', 'AUC_CI', 'AUPRC', 'AUPRC_CI', \n",
    "                    'F1', 'F1_CI', 'ACC', 'ACC_CI', 'MCC', 'MCC_CI',\n",
    "                    'Precision', 'Precision_CI', 'Recall', 'Recall_CI']\n",
    "    merged_df = merged_df[column_order]\n",
    "    \n",
    "    # 添加索引列\n",
    "    merged_df.insert(0, 'Dataset', new_index)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing k=30, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=30, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=30, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=30, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=31, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=31, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=31, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=31, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=32, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=32, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=32, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=32, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=33, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=33, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=33, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=33, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=34, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=34, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=34, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=34, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=35, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=35, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=35, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=35, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=36, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=36, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=36, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=36, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=37, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=37, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=37, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=37, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=38, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=38, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=38, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=38, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=39, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=39, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=39, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=39, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=40, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=40, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=40, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=40, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=41, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=41, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=41, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=41, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=42, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=42, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=42, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=42, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=43, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=43, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=43, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=43, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=44, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=44, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=44, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=44, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=45, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=45, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=45, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=45, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=46, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=46, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=46, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=46, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=47, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=47, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=47, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=47, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=48, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=48, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=48, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=48, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=49, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=49, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=49, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=49, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=50, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=50, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=50, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=50, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=51, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=51, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=51, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=51, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=52, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=52, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=52, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=52, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=53, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=53, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=53, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=53, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=54, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=54, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=54, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=54, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=55, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=55, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=55, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=55, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=56, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=56, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=56, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=56, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=57, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=57, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=57, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=57, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=58, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=58, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=58, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=58, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=59, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=59, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=59, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=59, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=60, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=60, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=60, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=60, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=61, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=61, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=61, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=61, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=62, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=62, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=62, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=62, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=63, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=63, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=63, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=63, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=64, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=64, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=64, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=64, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=65, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=65, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=65, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=65, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=66, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=66, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=66, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=66, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=67, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=67, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=67, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=67, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=68, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=68, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=68, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=68, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Processing k=69, classifier=logistic_regression\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "\n",
      "Processing k=69, classifier=decision_tree\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "\n",
      "Processing k=69, classifier=random_forest\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "\n",
      "Processing k=69, classifier=svm\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Results saved to 'model_evaluation_results_combined.xlsx'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_classifiers() -> Dict:\n",
    "    \"\"\"创建分类器字典\"\"\"\n",
    "    return {\n",
    "        'logistic_regression': LogisticRegression(class_weight='balanced'),\n",
    "        'decision_tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "        'random_forest': RandomForestClassifier(class_weight='balanced'),\n",
    "        'svm': SVC(class_weight='balanced', probability=True)\n",
    "    }\n",
    "\n",
    "def create_param_grids() -> Dict:\n",
    "    \"\"\"创建参数网格字典\"\"\"\n",
    "    return {\n",
    "        'logistic_regression': {\n",
    "            'classifier__C': [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "            'classifier__penalty': ['l1', 'l2'],  # 使用l1正则化\n",
    "            'classifier__max_iter': [2000],\n",
    "            'classifier__tol': [1e-4],\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'classifier__criterion': ['gini'],  # gini更常用\n",
    "            'classifier__max_depth': [3, 4, 5, 6],\n",
    "            'classifier__min_samples_split': [4, 5, 6, 7, 8],\n",
    "            'classifier__min_samples_leaf': [1, 2, 3]\n",
    "        },\n",
    "\n",
    "        'random_forest': {\n",
    "            'classifier__n_estimators': [90, 100, 110],         # 围绕100的核心取值\n",
    "            'classifier__max_depth': [4, 5, 6],                 # 中等深度范围，避免过拟合\n",
    "            'classifier__min_samples_split': [5, 6, 7],         # 适中的分裂条件\n",
    "            'classifier__min_samples_leaf': [1, 2],             # 常用的叶节点限制\n",
    "            'classifier__max_features': ['sqrt'],               # sqrt是最常用且效果好的选择\n",
    "            'classifier__max_samples': [0.8, 0.9],              # 主流的采样比例\n",
    "            'classifier__criterion': ['gini']                   # gini更常用且计算效率更高\n",
    "        },\n",
    "        \n",
    "        'svm': {\n",
    "            'classifier__C': [0.01, 0.05, 0.07, 0.1, 0.5, 1.0],  # 扩充正则化参数\n",
    "            'classifier__kernel': ['linear', 'rbf', 'poly'],  # 添加核函数选项\n",
    "            'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1]  # 扩充gamma选项\n",
    "        }\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, X: np.ndarray, y: np.ndarray, dataset_name: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    评估模型性能\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : 训练好的模型\n",
    "    X : array-like\n",
    "        特征矩阵\n",
    "    y : array-like\n",
    "        标签向量\n",
    "    dataset_name : str\n",
    "        数据集名称\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Optional[Dict] : 包含评估指标的字典，如果评估失败则返回None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "        metrics = calculate_metrics(y_proba, y)\n",
    "        metrics['dataset'] = dataset_name\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {dataset_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def run_grid_search(X_train: np.ndarray, y_train: np.ndarray, \n",
    "                   X_val: np.ndarray, y_val: np.ndarray, \n",
    "                   X_henan: np.ndarray, y_henan: np.ndarray, \n",
    "                   X_kits: np.ndarray, y_kits: np.ndarray, \n",
    "                   selected_classifiers: Optional[List[str]] = None, \n",
    "                   k_range: range = range(30, 70, 1)) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    运行网格搜索并评估模型\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : 训练数据\n",
    "    X_val, y_val : 验证数据\n",
    "    X_henan, y_henan : 河南数据集\n",
    "    X_kits, y_kits : KITS数据集\n",
    "    selected_classifiers : 要评估的分类器列表\n",
    "    k_range : 特征选择的k值范围\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : 评估结果表格\n",
    "    \"\"\"\n",
    "    classifiers = create_classifiers()\n",
    "    param_grids = create_param_grids()\n",
    "    results = []\n",
    "    \n",
    "    if selected_classifiers:\n",
    "        classifiers = {k: v for k, v in classifiers.items() if k in selected_classifiers}\n",
    "    \n",
    "    for k_num in k_range:\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            print(f\"\\nProcessing k={k_num}, classifier={clf_name}\")\n",
    "            \n",
    "            try:\n",
    "                pipeline = Pipeline([\n",
    "                    ('feature_selection', SelectKBest(mutual_info_classif, k=k_num)),\n",
    "                    ('classifier', clf)\n",
    "                ])\n",
    "                \n",
    "                grid_search = GridSearchCV(\n",
    "                    pipeline,\n",
    "                    param_grid=param_grids[clf_name],\n",
    "                    cv=5,\n",
    "                    # scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n",
    "                    refit='roc_auc',\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                grid_search.fit(X_train, y_train)\n",
    "                \n",
    "                base_result = {\n",
    "                    'k_num': k_num,\n",
    "                    'classifier': clf_name,\n",
    "                    'best_params': str(grid_search.best_params_),\n",
    "                    'best_score': grid_search.best_score_\n",
    "                }\n",
    "                \n",
    "                datasets = {\n",
    "                    'validation': (X_val, y_val),\n",
    "                    'henan': (X_henan, y_henan),\n",
    "                    'kits': (X_kits, y_kits)\n",
    "                }\n",
    "                \n",
    "                for dataset_name, (X, y) in datasets.items():\n",
    "                    metrics = evaluate_model(grid_search.best_estimator_, X, y, dataset_name)\n",
    "                    if metrics:\n",
    "                        result = base_result.copy()\n",
    "                        result.update(metrics)\n",
    "                        results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {clf_name} with k={k_num}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    results_df.to_excel('model_evaluation_results_combined.xlsx', index=False)\n",
    "    print(\"\\nResults saved to 'model_evaluation_results_combined.xlsx'\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# 使用示例\n",
    "results_df = run_grid_search(X_train, y_train, X_val, y_val, X_henan, y_henan, X_kits, y_kits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': 0.996, 'AUC_CI': (0.9923387096774193, 0.99868454825462), 'F1': 0.833, 'F1_CI': (0.7499999999999999, 0.9038461538461537), 'ACC': 0.967, 'ACC_CI': (0.9528130671506352, 0.9818511796733213), 'MCC': 0.822, 'MCC_CI': (0.733363247144095, 0.8951919642429947), 'AUPRC': 0.968, 'AUPRC_CI': (0.9360269465544129, 0.9895943882288031), 'Precision': 0.938, 'Precision_CI': (0.8571428571428571, 1.0), 'Recall': 0.75, 'Recall_CI': (0.6363636363636364, 0.8596491228070176)}\n",
      "{'AUC': 0.8, 'AUC_CI': (0.7033726127590411, 0.8842998585572843), 'F1': 0.25, 'F1_CI': (0.05555555555555555, 0.4444444444444444), 'ACC': 0.899, 'ACC_CI': (0.8607594936708861, 0.9367088607594937), 'MCC': 0.287, 'MCC_CI': (0.0379038020032827, 0.48012444274321275), 'AUPRC': 0.388, 'AUPRC_CI': (0.20399836083211706, 0.5729014130041367), 'Precision': 0.667, 'Precision_CI': (0.2, 1.0), 'Recall': 0.154, 'Recall_CI': (0.03125, 0.3103448275862069)}\n",
      "{'AUC': 0.693, 'AUC_CI': (0.5954022988505747, 0.7807585568917669), 'F1': 0.24, 'F1_CI': (0.08163265306122448, 0.3902439024390244), 'ACC': 0.797, 'ACC_CI': (0.7379679144385026, 0.8502673796791443), 'MCC': 0.294, 'MCC_CI': (0.11294530761424883, 0.43706825997337895), 'AUPRC': 0.453, 'AUPRC_CI': (0.3094281289222726, 0.6111561363171343), 'Precision': 0.857, 'Precision_CI': (0.5, 1.0), 'Recall': 0.14, 'Recall_CI': (0.044444444444444446, 0.25)}\n",
      "{'AUC': 0.676, 'AUC_CI': (0.5641025641025641, 0.7827586206896552), 'F1': 0.171, 'F1_CI': (0.0, 0.33333333333333337), 'ACC': 0.826, 'ACC_CI': (0.7664670658682635, 0.8802395209580839), 'MCC': 0.227, 'MCC_CI': (-0.03179681046292959, 0.400045722123942), 'AUPRC': 0.422, 'AUPRC_CI': (0.24605617186016998, 0.5996843524739349), 'Precision': 0.75, 'Precision_CI': (0.0, 1.0), 'Recall': 0.097, 'Recall_CI': (0.0, 0.20833333333333334)}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 读取proba和gt\n",
    "# data = pd.read_excel('./result/ring.xlsx')\n",
    "data = pd.read_excel(\"./result/combined.xlsx\")\n",
    "\n",
    "y_train = data[data['dataset'] == 'train']['ground_truth']\n",
    "y_val = data[data['dataset'] == 'val']['ground_truth']\n",
    "y_henan = data[data['dataset'] == 'henan']['ground_truth']\n",
    "y_kits = data[data['dataset'] == 'kits']['ground_truth']\n",
    "\n",
    "proba_train = data[data['dataset'] == 'train']['probability']\n",
    "proba_val = data[data['dataset'] == 'val']['probability']\n",
    "proba_henan = data[data['dataset'] == 'henan']['probability']\n",
    "proba_kits = data[data['dataset'] == 'kits']['probability']\n",
    "\n",
    "# 计算评估指标\n",
    "result_train = calculate_metrics(proba_train, y_train)\n",
    "result_val = calculate_metrics(proba_val, y_val)\n",
    "result_henan = calculate_metrics(proba_henan, y_henan)\n",
    "result_kits = calculate_metrics(proba_kits, y_kits)\n",
    "print(result_train)\n",
    "print(result_val)\n",
    "print(result_henan)\n",
    "print(result_kits)\n",
    "\n",
    "# 合并字典\n",
    "merged_results = merge_dicts_to_df(result_train, result_val, result_henan, result_kits)\n",
    "\n",
    "# 保存为CSV文件\n",
    "merged_results.to_excel('results_combined.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUC': 0.996, 'AUC_CI': (0.9925762910798123, 0.9989177489177489), 'F1': 0.833, 'F1_CI': (0.75, 0.9027777777777779), 'ACC': 0.967, 'ACC_CI': (0.9509981851179673, 0.9818511796733213), 'MCC': 0.822, 'MCC_CI': (0.739529416616842, 0.8939145317544533), 'AUPRC': 0.968, 'AUPRC_CI': (0.9362304422337879, 0.9913322753883128), 'Precision': 0.938, 'Precision_CI': (0.8636363636363636, 1.0), 'Recall': 0.75, 'Recall_CI': (0.6333333333333333, 0.8529411764705882)}\n",
      "{'AUC': 0.697, 'AUC_CI': (0.5871621621621621, 0.8033826638477801), 'F1': 0.054, 'F1_CI': (0.0, 0.17647058823529413), 'ACC': 0.852, 'ACC_CI': (0.8059071729957806, 0.8945147679324894), 'MCC': -0.013, 'MCC_CI': (-0.09028873380114831, 0.12538960619191256), 'AUPRC': 0.196, 'AUPRC_CI': (0.11349424735101599, 0.3111131542391209), 'Precision': 0.091, 'Precision_CI': (0.0, 0.3076923076923077), 'Recall': 0.038, 'Recall_CI': (0.0, 0.13636363636363635)}\n",
      "{'AUC': 0.604, 'AUC_CI': (0.502791461412151, 0.7034188034188034), 'F1': 0.044, 'F1_CI': (0.0, 0.15), 'ACC': 0.77, 'ACC_CI': (0.7112299465240641, 0.839572192513369), 'MCC': 0.067, 'MCC_CI': (-0.06281362424511791, 0.23324405648732158), 'AUPRC': 0.322, 'AUPRC_CI': (0.21697867813280827, 0.46117434700424875), 'Precision': 0.5, 'Precision_CI': (0.0, 1.0), 'Recall': 0.023, 'Recall_CI': (0.0, 0.08333333333333333)}\n",
      "{'AUC': 0.618, 'AUC_CI': (0.4976675148430874, 0.7365445499773858), 'F1': 0.114, 'F1_CI': (0.0, 0.25806451612903225), 'ACC': 0.814, 'ACC_CI': (0.7544910179640718, 0.874251497005988), 'MCC': 0.127, 'MCC_CI': (-0.06417617007657414, 0.3156687733897673), 'AUPRC': 0.314, 'AUPRC_CI': (0.18453931334570078, 0.5133943903936442), 'Precision': 0.5, 'Precision_CI': (0.0, 1.0), 'Recall': 0.065, 'Recall_CI': (0.0, 0.15625)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "classifier = RandomForestClassifier(\n",
    "n_estimators=90,\n",
    "max_depth=6,\n",
    "min_samples_split=5,\n",
    "min_samples_leaf=2,\n",
    "max_features='sqrt',\n",
    "max_samples=0.8,\n",
    "random_state=42,  # 添加随机种子以确保可重复性\n",
    "n_jobs=-1,  # 使用所有CPU核心\n",
    "class_weight='balanced'\n",
    ")\n",
    "\n",
    "# 2. 构建pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectKBest(mutual_info_classif, k=45)),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# 3. 训练模型\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. 在训练集和内部验证集上评估\n",
    "train_proba = pipeline.predict_proba(X_train)[:, 1]\n",
    "val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 5. 在两个测试集上评估\n",
    "henan_proba = pipeline.predict_proba(X_henan)[:, 1]\n",
    "kits_proba = pipeline.predict_proba(X_kits)[:, 1]\n",
    "\n",
    "# 将计算指标转为表格\n",
    "train_df = pd.DataFrame({\n",
    "    'dataset': 'train',\n",
    "    'name': X_train.index,\n",
    "    'ground_truth': y_train,\n",
    "    'probability': train_proba\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    'dataset': 'val', \n",
    "    'name': X_val.index,\n",
    "    'ground_truth': y_val,\n",
    "    'probability': val_proba\n",
    "})\n",
    "\n",
    "henan_df = pd.DataFrame({\n",
    "    'dataset': 'henan',\n",
    "    'name': X_henan.index,\n",
    "    'ground_truth': y_henan,\n",
    "    'probability': henan_proba\n",
    "})\n",
    "\n",
    "kits_df = pd.DataFrame({\n",
    "    'dataset': 'kits',\n",
    "    'name': X_kits.index,\n",
    "    'ground_truth': y_kits,\n",
    "    'probability': kits_proba\n",
    "})\n",
    "# 合并所有DataFrame\n",
    "final_df = pd.concat([train_df, val_df, henan_df, kits_df], ignore_index=True)\n",
    "# 保存为Excel文件\n",
    "final_df.to_excel('combined.xlsx', index=False)\n",
    "\n",
    "# 6. 计算各项指标\n",
    "result_train = calculate_metrics(train_proba, y_train)\n",
    "result_val = calculate_metrics(val_proba, y_val)\n",
    "result_henan = calculate_metrics(henan_proba, y_henan)\n",
    "result_kits = calculate_metrics(kits_proba, y_kits)\n",
    "print(result_train)\n",
    "print(result_val)\n",
    "print(result_henan)\n",
    "print(result_kits)\n",
    "\n",
    "# 合并字典\n",
    "merged_results = merge_dicts_to_df(result_train, result_val, result_henan, result_kits)\n",
    "\n",
    "# 保存为CSV文件\n",
    "merged_results.to_excel('results_combined.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (roc_curve, precision_recall_curve, average_precision_score,\n",
    "                           roc_auc_score, confusion_matrix, classification_report)\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def plot_classifier_evaluation_separate(y_true, y_prob, save_path=None):\n",
    "    \"\"\"\n",
    "    分别绘制并保存分类器评估图\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实标签 (0/1)\n",
    "    y_prob: 预测概率\n",
    "    save_path: PDF保存路径\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ROC曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    plt.plot(fpr, tpr, 'b-', label=f'ROC (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_roc.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. PR曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    plt.plot(recall, precision, 'b-', label=f'PR (AP = {ap:.3f})')\n",
    "    plt.axhline(y=sum(y_true)/len(y_true), color='r', linestyle='--', label='Random')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_pr.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. 预测概率分布\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label, color in zip([0, 1], ['red', 'blue']):\n",
    "        mask = y_true == label\n",
    "        plt.hist(y_prob[mask], bins=50, density=True, alpha=0.5, \n",
    "                color=color, label=f'Class {label}')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Probability Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_prob_dist.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. 分类阈值分析\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    scores = []\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_prob >= thresh).astype(int)\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        scores.append([accuracy, precision, recall, f1])\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.plot(thresholds, scores[:, i], label=metric)\n",
    "    plt.xlabel('Classification Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metric Scores vs Classification Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_threshold.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. 校准曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, 's-', label='Calibration curve')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Perfect calibration')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_calibration.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. 混淆矩阵\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}_confusion.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # # 7. 分类瀑布图\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # sorted_indices = np.argsort(y_prob)\n",
    "    # sorted_probs = y_prob[sorted_indices]\n",
    "    # sorted_true = y_true[sorted_indices]\n",
    "    \n",
    "    # plt.plot(range(len(sorted_probs)), sorted_probs, 'b-', label='Predicted probability')\n",
    "    # plt.scatter(range(len(sorted_true)), sorted_true, c='r', alpha=0.5, \n",
    "    #            label='True label', marker='.')\n",
    "    # plt.axhline(y=0.5, color='g', linestyle='--', label='Decision threshold')\n",
    "    # plt.xlabel('Samples (sorted by predicted probability)')\n",
    "    # plt.ylabel('Probability / Class')\n",
    "    # plt.title('Classification Waterfall Plot')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # if save_path:\n",
    "    #     plt.savefig(f'{save_path}_waterfall.pdf')\n",
    "    # plt.close()\n",
    "    \n",
    "    # return {\n",
    "    #     'AUC': auc,\n",
    "    #     'AP': ap,\n",
    "    #     'Confusion Matrix': cm,\n",
    "    #     'Classification Report': classification_report(y_true, y_pred)\n",
    "    # }\n",
    "\n",
    "# 使用示例:\n",
    "plot_classifier_evaluation_separate(y_val, val_proba, save_path='inter_test')\n",
    "plot_classifier_evaluation_separate(y_henan, henan_proba, save_path='henan_test')\n",
    "plot_classifier_evaluation_separate(y_kits, kits_proba, save_path='kits_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有图像和数据已保存到目录: feature_importance_results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shap\n",
    "def save_feature_importance_plots(pipeline, X_train, y_train, X_val, y_val, feature_names, save_dir='feature_importance_plots'):\n",
    "    \"\"\"\n",
    "    保存所有特征重要性相关的图为PDF\n",
    "    \"\"\"\n",
    "    # 创建保存目录\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    # 1. 特征选择分析图\n",
    "    selector = pipeline.named_steps['feature_selection']\n",
    "    selected_mask = selector.get_support()\n",
    "    selected_features = feature_names[selected_mask]\n",
    "    scores = selector.scores_\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(selected_features)), \n",
    "            pd.DataFrame({'Feature': feature_names, 'Selected': selected_mask, \n",
    "                         'Mutual_Info_Score': scores})[selected_mask]['Mutual_Info_Score'])\n",
    "    plt.xticks(range(len(selected_features)), selected_features, rotation=45, ha='right')\n",
    "    plt.title('Mutual Information Scores of Selected Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'mutual_info_scores.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Random Forest特征重要性图\n",
    "    rf_model = pipeline.named_steps['classifier']\n",
    "    importances = rf_model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': importances,\n",
    "        'Std': std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    feature_importance.plot(kind='bar', x='Feature', y='Importance', yerr='Std',\n",
    "                          capsize=5, alpha=0.8)\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'rf_importance.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. SHAP值分析图\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        X_selected = X_train.iloc[:, selected_mask].values\n",
    "    else:\n",
    "        X_selected = X_train[:, selected_mask]\n",
    "    \n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    shap_values = explainer.shap_values(X_selected)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # SHAP summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_selected, \n",
    "                     feature_names=selected_features.tolist(),\n",
    "                     show=False)\n",
    "    plt.title('SHAP Summary Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'shap_summary.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # SHAP依赖图（top 3特征）\n",
    "    mean_abs_shap = np.abs(shap_values).mean(0)\n",
    "    top_features_idx = np.argsort(mean_abs_shap)[-3:]\n",
    "    \n",
    "    for idx in top_features_idx:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        shap.dependence_plot(idx, shap_values, X_selected,\n",
    "                           feature_names=selected_features.tolist(),\n",
    "                           show=False)\n",
    "        plt.title(f'SHAP Dependence Plot - {selected_features[idx]}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f'shap_dependence_{selected_features[idx]}.pdf'))\n",
    "        plt.close()\n",
    "        \n",
    "    # 4. 合并所有重要性指标并保存\n",
    "    final_importance = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Mutual_Info_Score': scores[selected_mask],\n",
    "        'RF_Importance': importances,\n",
    "        'RF_Importance_Std': std,\n",
    "        'Mean_Abs_SHAP': mean_abs_shap\n",
    "    })\n",
    "    \n",
    "    final_importance.to_csv(os.path.join(save_dir, 'feature_importance_metrics.csv'), index=False)\n",
    "    \n",
    "    print(f\"所有图像和数据已保存到目录: {save_dir}\")\n",
    "    return final_importance\n",
    "\n",
    "# 使用示例:\n",
    "results = save_feature_importance_plots(\n",
    "    pipeline=pipeline,\n",
    "    X_train=X_train, \n",
    "    y_train=y_train,\n",
    "    X_val=X_val, \n",
    "    y_val=y_val,\n",
    "    feature_names=np.array(X_train.columns),\n",
    "    save_dir='feature_importance_results'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
